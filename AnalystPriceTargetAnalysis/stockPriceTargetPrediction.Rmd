---
title: "Analyst Price Target Analysis using GML Logistic regression model "
output: html_document
---

```{r setup, include=FALSE}
library(rvest)
library(stringr)
library(tidyr)
library(dummies)
library(quantmod)
library(caret)
library(ROCR)
```


```{r Load and Drop}
#Drop Cols that will not be used in model
finaldf <- read.csv("AnalystRatingData/ratings.csv", check.names = TRUE)
drops <- c("Date","Firm","Action","Rating","Ticker","", "Price.After.One.Year", "Price.Target")
finaldf <- finaldf[ , !(names(finaldf) %in% drops)]

```


```{r initial}
#apply factors to columns that have 0 and 1 and as.numeric to columns that are numbers
factor_cols <- -which(names(finaldf) %in% c('Price Target','Price After One Year'))
finaldf[factor_cols] <- lapply(finaldf[factor_cols],factor)
numeric_cols <-c('Price.Target','Price.After.One.Year')
finaldf[numeric_cols] <- lapply(finaldf[numeric_cols],as.numeric)


```

```{r zero_variance}
nzv <- nearZeroVar(finaldf,saveMetrics = TRUE)
sorted <-nzv[order(nzv$freqRatio),]
print(sorted)

#remove features that with high freq ratio (which appears less frequently.). freqRatio of 700 means it appears once in 700 times. This 
#helped us to remove some of analyst/banks that are less known and 
remove_features <- rownames(nzv[nzv$freqRatio > 700, ])
#this reduced number of columns from 137 to 94
finaldf <-finaldf[,!names(finaldf)%in% remove_features]

```

```{r split data}
 
set.seed(417)
#make.name will replace space with dot in column names
names(finaldf)<-make.names(names(finaldf),unique=TRUE)
finaldf<-x<-na.omit(finaldf)
inTrain <- createDataPartition(y=finaldf$"TargetAchieved",p = 0.7, list=FALSE)
training <- finaldf[inTrain,]
testing <- finaldf[-inTrain,]
```

```{r glm}
system.time(
  model <-glm(TargetAchieved ~ ., data=testing, family = "binomial")
)
summary(model)
```


```{r 10}
glm.probs <-predict(model,testing,type = "response")
```


```{r Cuttoff and confustion matrix}
#this creates a list with all 0 with same size as testing
glm.predict <- rep( 0 ,nrow(testing))

glm.predict[glm.probs > .5] <- 1 #cutoff set to 0.5
confusionMatrix(glm.predict, testing$TargetAchieved, positive ="1", mode="prec_recall")
```

```{r ROC}
pred <-prediction(glm.probs,testing$TargetAchieved)
perf <- performance(pred,"tpr","fpr")
plot(perf, colorize=TRUE)
abline(a=0, b=1)
```

```{r AUC}
performance(pred,measure = "auc")@y.values
```

```{r Selecting Threshold}
dfLength <- length(seq(0.00,0.999,0.001))
thresholdDF <- data.frame(percent = numeric(dfLength),threshold = numeric(dfLength),
precision = numeric(dfLength), recall = numeric(dfLength), f1 = numeric(dfLength))
dfListIndex <-0
for (i in seq(0.00, 0.999, 0.001)){
dfListIndex = dfListIndex +1
#get minimum value from the sorted list which will be our cutoff value
threshold = min(head(sort(glm.probs,decreasing=TRUE), n = (length(glm.probs)*(1-i))
))
thresholdDF$percent[dfListIndex] = (1-i)*100
thresholdDF$threshold[dfListIndex] = threshold
# set to 1 if probability is above threshold otehr wise 0
glm.pred = rep(0, nrow(testing))
glm.pred[glm.probs >= threshold] = 1
#create confustion matrix and get pos pred value from the confusion matrix
cm = confusionMatrix(glm.pred, testing$TargetAchieved, positive="1", mode="prec_recall")
byClass = cm$byClass
precision = byClass[5]
recall = byClass[6]
f1 = byClass[7]
thresholdDF$precision[dfListIndex] = precision
thresholdDF$recall[dfListIndex] = recall
thresholdDF$f1[dfListIndex] = f1
}

# Plot
ggplot(data=thresholdDF,aes(threshold, y=value,color =variable))+ geom_point(aes(y=precision , col = "Precision"))+ geom_point(aes(y=recall , col = "Recall"))+ geom_point (aes(y=f1 , col = "F1 (Harmonic Mean of Precision & Recall)"))

```

```{r ROOCR}
perf <- performance(pred,"prec","rec")
plot(perf, colorize=TRUE)
```


```{r Variable importance}
library(tibble)
imp <- varImp(model, scale=FALSE)
imp <- rownames_to_column(imp)
# find top N most important variables
print(imp[order(imp$Overall, decreasing=TRUE)[1:25], ])
```

